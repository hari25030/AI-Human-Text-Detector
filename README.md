AI vs Human Text Detector
Machine and Deep Learning Classification Web Application with Streamlit
Project Overview

This project delivers a live web application capable of analyzing text from various sources (MS Word documents, PDF files, or direct text input) to determine whether it was written by a human or generated by an Artificial Intelligence. The system provides intuitive probability scores indicating the likelihood of AI versus human authorship, offering crucial insights in an increasingly AI-driven content landscape.

Key Features

Flexible Text Input: Users can easily upload .docx or .pdf files, or type/paste text directly into the application.

Multiple Model Selection: Choose from a diverse set of trained classifiers, including traditional Machine Learning models (SVM, Decision Tree, AdaBoost) and advanced Deep Learning Classifiers (CNN, LSTM, RNN).

Instant Predictions: Get real-time AI vs. Human classification results with clear confidence scores.

Visualizations & Insights:

Feature Importance: For ML models, visualize the top words that influenced the prediction.

Text Statistics: View key metrics like word count, sentence count, and readability.

Model Comparison: See a side-by-side performance comparison of what each model predicts for the current text.

Comprehensive Reports: Download a detailed PDF report summarizing the analysis, including the prediction, text statistics, and overall model performance.

Technical Stack

Main Framework: Streamlit (for interactive web application)

Machine Learning: scikit-learn (SVM, Decision Tree, AdaBoost)

Deep Learning: PyTorch (CNN, LSTM, RNN architectures)

Data Processing: pandas, numpy

Document Processing: PyPDF2 (for PDF text extraction), python-docx (for Word document processing)

Text Preprocessing: re (regular expressions), nltk (for stopwords and lemmatization in notebook)

Visualization: matplotlib, seaborn (for static plots)

PDF Generation: reportlab

Model Serialization: joblib, pickle

Project Structure

The repository is organized as follows:

AI-Human-Text-Detector/
├── app.py              # Main Streamlit web application code
├── requirements.txt    # Python package dependencies for the project
├── packages.txt        # System-level dependencies required for deployment (e.g., libgomp1)
├── models/             # Directory containing all trained ML and DL models
│   ├── svm_model.joblib        # Trained Support Vector Machine model
│   ├── decision_tree_model.joblib # Trained Decision Tree model
│   ├── adaboost_model.joblib   # Trained AdaBoost model
│   ├── cnn_model.pth           # Trained Convolutional Neural Network model (PyTorch)
│   ├── lstm_model.pth          # Trained Long Short-Term Memory model (PyTorch)
│   ├── rnn_model.pth           # Trained Recurrent Neural Network model (PyTorch)
│   ├── tokenizer.pkl           # Keras Tokenizer for Deep Learning models
│   └── vectorizer.joblib       # TF-IDF Vectorizer for Machine Learning models
├── data/               # Directory for datasets
│   └── training_data/  # Contains the primary dataset used for model training
│       └── AI_vs_huam_train_dataset.xlsx
│   └── test_data/      
├── notebooks/          # Jupyter Notebooks used for development, training, and evaluation
│   └── AI_vs_Human_Text_Classifierfinal_ipynb_Deep_Learning_&_ML_Training_Notebook.ipynb # Main project notebook with data analysis, model development, and evaluation
└── README.md           # This project documentation file


Setup and Local Run Instructions

To set up and run this project locally, follow these steps. Please note that due to the complex dependencies of Deep Learning libraries (PyTorch, TensorFlow) and potential compilation issues on certain operating systems (especially macOS), local execution can be challenging. For the most reliable experience, please use the live Streamlit Cloud deployment.

Clone the Repository:

git clone https://github.com/hari25030/AI-Human-Text-Detector.git
cd AI-Human-Text-Detector


Create and Activate a Python Virtual Environment (Recommended):

python3 -m venv venv
# On macOS/Linux:
source venv/bin/activate
# On Windows:
.\venv\Scripts\activate


Install Python Dependencies:
Install all required Python packages using pip:

pip install -r requirements.txt


Install System Dependencies (Important for Linux/Streamlit Cloud):
The packages.txt file is used by Streamlit Cloud to install necessary system libraries. If running on a local Linux machine (including WSL), you must manually install libgomp1 to avoid errors, as some underlying libraries (like scikit-learn's joblib) depend on it:

# For Debian/Ubuntu-based systems:
sudo apt-get update && sudo apt-get install -y libgomp1


(This step is automatically handled by Streamlit Cloud during deployment.)

Ensure Model and Data Files are Present:
It is critical that all trained models (.joblib, .pth, .pkl files) are in the models/ directory and your training data (AI_vs_huam_train_dataset.xlsx) is in data/training_data/. These files are included in the repository, so ensure they are present after cloning. Local execution issues often arise if these files are missing or in the wrong location.

Run the Streamlit Application:
Once all dependencies are installed and files are in place, you can run the Streamlit app:

streamlit run app.py


This will open the application in your default web browser.

Deployment Challenges & Solutions

During the development and deployment of this project, several challenges were encountered, primarily related to making the Streamlit web application publicly accessible from a cloud environment (Google Colab) and ensuring local environment compatibility.

Local Execution Issues: Running the Streamlit app directly via the terminal proved challenging on local machines, particularly macOS. This was largely due to the complex binary dependencies of Deep Learning libraries (PyTorch, TensorFlow) and pyarrow, which often failed to compile from source or caused version conflicts.

Tunneling Service Unreliability: Initial attempts to expose the Colab-hosted Streamlit app using tunneling services like localtunnel and ngrok encountered persistent issues. These included frequent password prompts, authentication failures, and general instability, making them unreliable for a consistent public demo.

GitHub Repository Visibility: A key hurdle for Streamlit Cloud deployment was ensuring the GitHub repository was set to Public, as the free tier requires public access to clone and build the application.

File Path and Type Specificity: Streamlit Cloud's deployment interface required precise GitHub URLs (pointing directly to app.py without tokens) and correct file types (e.g., packages.txt as plain text, not .rtf).

Solution:

The most robust and successful solution for deploying this application was leveraging Streamlit Cloud. This platform handles the complexities of environment setup, dependency installation (including system-level packages via packages.txt), and public hosting directly from the GitHub repository. This approach bypassed the local compilation and tunneling issues, providing a stable and easily shareable live web application.

Live Application / Deployment

This application is deployed live on Streamlit Cloud for easy access and demonstration. For the most reliable experience, it is highly recommended to use the live Streamlit Cloud deployment.

Demo Video

A concise demo video showcasing the application's key features, user interface, and predictions on sample texts is available.